# tests/test_phase22_interpreter.py

import pytest
from pathlib import Path
import sys
from unittest.mock import MagicMock, patch

# Ensure the core modules can be imported
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from core.idea_interpreter import IdeaInterpreter
from core.llm_provider_base import LLMProvider
from core.user_profile import UserProfile
from core.memory import Memory
from core.style_preference import StylePreferenceManager
from core.project_contextualizer import ProjectContextualizer

# --- Fixtures ---

@pytest.fixture
def mock_interpreter_dependencies():
    """Provides a dictionary of mocked dependencies for the IdeaInterpreter."""
    mock_llm = MagicMock(spec=LLMProvider)
    mock_llm.is_available.return_value = True
    
    mock_user_profile = MagicMock(spec=UserProfile)
    # Mock the return of get_preference to avoid issues with unconfigured profiles
    mock_user_profile.get_preference.return_value = "Test User"
    
    mock_style_manager = MagicMock(spec=StylePreferenceManager)
    mock_style_manager.get_preference.return_value = "professional"

    mock_contextualizer = MagicMock(spec=ProjectContextualizer)
    mock_contextualizer.get_full_context.return_value = "No existing project context."

    return {
        "llm_provider": mock_llm,
        "user_profile": mock_user_profile,
        "memory": MagicMock(spec=Memory),
        "style_manager": mock_style_manager,
        "project_contextualizer": mock_contextualizer,
    }

# --- Evaluation for Task 22.1, 22.2, 22.3: Interactive Interpreter ---

def test_interpreter_starts_session_and_asks_questions(mock_interpreter_dependencies):
    """
    Assesses if the IdeaInterpreter can start a session and generate initial questions.
    """
    interpreter = IdeaInterpreter(**mock_interpreter_dependencies)
    mock_llm = mock_interpreter_dependencies["llm_provider"]
    
    # Mock the LLM's response to the initial prompt
    expected_questions = "1. What is the main goal?\n2. Who is the target audience?"
    mock_llm.generate_text.return_value = expected_questions

    questions = interpreter.start_interpretation_session("A new social media app")

    mock_llm.generate_text.assert_called_once()
    assert questions == expected_questions, "The method should return the questions generated by the LLM."
    assert interpreter.is_interpreting is True, "The session should be marked as active."
    assert len(interpreter.conversation_history) > 0, "Conversation history should be initiated."

def test_interpreter_continues_and_completes_session(mock_interpreter_dependencies):
    """
    Assesses the full interview flow from answering questions to getting a final brief.
    """
    interpreter = IdeaInterpreter(**mock_interpreter_dependencies)
    mock_llm = mock_interpreter_dependencies["llm_provider"]

    # --- Simulate the multi-call LLM interaction ---
    # First call (for questions)
    initial_questions = "What is the primary feature?"
    # Second call (for synthesis)
    final_brief_summary = "This is the final synthesized brief based on the conversation."
    
    # The llm_synthesized_summary field in the final dict uses the LLM output directly.
    interpreter.current_brief = {
        "title": "Synthesized Project Title",
        "summary": "A brief summary.",
        "llm_synthesized_summary": final_brief_summary
    }

    mock_llm.generate_text.side_effect = [
        initial_questions, 
        final_brief_summary 
    ]

    # 1. Start the session
    interpreter.start_interpretation_session("A new music app")
    
    # 2. Submit an answer
    user_answer = "The primary feature is AI-powered playlist generation."
    result = interpreter.submit_answer_and_continue(user_answer)
    
    # 3. Verify the final result
    assert result["status"] == "complete", "The session should complete after one answer (in the simplified flow)."
    assert result["type"] == "brief", "The result type should be a 'brief'."
    assert interpreter.is_interpreting is False, "The session should be marked as inactive after completion."
    
    final_brief = result["data"]
    assert "title" in final_brief, "The final brief must have a title."
    assert final_brief["llm_synthesized_summary"] == final_brief_summary, "The brief should contain the synthesized summary from the LLM."
    
    # Check that the LLM was called twice (once for questions, once for synthesis)
    assert mock_llm.generate_text.call_count == 2, "LLM should be called for questions and then for synthesis."

